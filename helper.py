import ast
import csv
import os

import numpy as np
import pandas as pd
import stanza
from sentence_transformers import SentenceTransformer


def calc_metric(true_hpos, method_hpos):
    """
    Calculate precision, recall, and F1 score for two sets of HPO terms.

    Parameters:
    -----------
    true_hpos : set
        The set of ground truth HPO terms.
    method_hpos : set
        The set of HPO terms predicted or generated by a method.

    Returns:
    --------
    tuple of floats
        A tuple (precision, recall, f1_score), where:
        - precision is the proportion of correctly predicted terms out of all predicted terms,
        - recall is the proportion of correctly predicted terms out of all true terms,
        - f1_score is the harmonic mean of precision and recall.

    Notes:
    ------
    - If both input sets are empty, the function returns precision, recall, and F1 score of 1.
    - If either set is empty (and the other is not), precision, recall, and F1 score will be zero.
    """
    pr = 0
    re = 0
    f1 = 0
    if len(true_hpos) == 0 and len(method_hpos) == 0:
        pr = 1
        re = 1
        f1 = 1
    elif len(true_hpos) != 0 and len(method_hpos) != 0:
        pr = len(true_hpos & method_hpos) / len(method_hpos)
        re = len(true_hpos & method_hpos) / len(true_hpos)
        if pr + re > 0:
            f1 = 2 * pr * re / (pr + re)
    return pr, re, f1


def evaluate_micro_macro(ground_truth_sets, predicted_sets):
    """
    Evaluates precision, recall, and F1 score on given ground truth and predicted sets using micro and macro evaluation.

    Parameters:
    - ground_truth_sets (list of set): List of sets containing ground truth HPO codes for each sample.
    - predicted_sets (list of set): List of sets containing predicted HPO codes for each sample.

    Returns:
    - dict: A dictionary with micro and macro precision, recall, and F1 scores.
    - dict: A dictionary with precision, recall, and F1 score for each sample.
    """
    true_positive_count = 0
    predicted_positive_count = 0
    actual_positive_count = 0

    sum_recall = 0
    sum_precision = 0

    # Per-sample results
    individual_results = {"precision": [], "recall": [], "F1": []}

    for true_hpos, pred_hpos in zip(ground_truth_sets, predicted_sets):
        pr, re, f1 = calc_metric(true_hpos, pred_hpos)
        true_positive_count += len(true_hpos & pred_hpos)
        predicted_positive_count += len(pred_hpos)
        actual_positive_count += len(true_hpos)
        sum_recall += re
        sum_precision += pr

        individual_results["precision"].append(pr)
        individual_results["recall"].append(re)
        individual_results["F1"].append(f1)

    # Calculate micro metrics
    micro_precision = true_positive_count / predicted_positive_count
    micro_recall = true_positive_count / actual_positive_count
    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall)

    # Calculate macro metrics
    macro_precision = sum_precision / len(ground_truth_sets)
    macro_recall = sum_recall / len(ground_truth_sets)
    macro_f1 = 2 * macro_precision * macro_recall / (macro_precision + macro_recall)

    # Return a summary of results
    metrics = {
        "micro_precision": micro_precision,
        "micro_recall": micro_recall,
        "micro_f1": micro_f1,
        "macro_precision": macro_precision,
        "macro_recall": macro_recall,
        "macro_f1": macro_f1,
    }

    return metrics, individual_results


def evaluate_UTI(symptomatic_patients, patient_data):
    """
    Evaluate the performance of UTI (Urinary Tract Infection) prediction based on symptomatic patients.

    Parameters:
    -----------
    symptomatic_patients : iterable
        A collection of patient identifiers predicted to be symptomatic (positive prediction for UTI).
    patient_data : list of tuples
        Each tuple contains patient data in the form (patient_id, diagnosis_label), where
        diagnosis_label is expected to be one of:
        - "UTI" or "CAUTI" (indicating true positive UTI cases),
        - "AB" (indicating asymptomatic bacteriuria, used as true negatives).

    Returns:
    --------
    tuple:
        wrong : dict
            A dictionary with two keys:
            - "false neg": list of patient IDs who have UTI but were not predicted symptomatic,
            - "false pos": list of patient IDs who do not have UTI but were predicted symptomatic.
        results : dict
            A dictionary containing performance metrics:
            - "Accuracy": overall accuracy of the predictions,
            - "Precision": proportion of true positive predictions out of all positive predictions,
            - "Recall": proportion of true positive cases correctly predicted,
            - "F1": harmonic mean of precision and recall.
    """
    pred_pos = set(symptomatic_patients)
    pred_neg = set([entry[0] for entry in patient_data if entry[0] not in pred_pos])

    true_pos = set([entry[0] for entry in patient_data if entry[1] == "UTI" or entry[1] == "CAUTI"])
    true_neg = set([entry[0] for entry in patient_data if entry[1] == "AB"])

    acc = (len(pred_pos.intersection(true_pos)) + len(pred_neg.intersection(true_neg))) / len(patient_data)
    precision = len(pred_pos.intersection(true_pos)) / len(pred_pos)
    recall = len(true_pos.intersection(pred_pos)) / len(true_pos)
    F1 = 2 * precision * recall / (precision + recall)
    results = {"Accuracy": acc, "Precision": precision, "Recall": recall, "F1": F1}
    wrong = {}
    wrong["false neg"] = [patient for patient in true_pos if patient not in pred_pos]
    wrong["false pos"] = [patient for patient in true_neg if patient not in pred_neg]
    return wrong, results


def load_txt(context_dir):
    """
    Load text files from a directory into a dictionary, using processed filenames as keys.

    Parameters:
    -----------
    context_dir : str
        Path to the directory containing text files to be loaded.

    Returns:
    --------
    dict
        A dictionary where each key is the filename (without extension) with underscores replaced by colons,
        and each value is the full content of the corresponding text file read with 'latin1' encoding.
    """
    context_dict = {}
    files = [file for file in os.listdir(context_dir) if file.endswith(".txt")]
    for file in files:
        with open(os.path.join(context_dir, file), "r", encoding="latin1") as f:
            content = f.read()
        context_dict[file.split(".")[0].replace("_", ":")] = content
    return context_dict


def prepare_context(context_dict):
    """
    Process the text content in a context dictionary by removing newlines and splitting by '*'.

    Parameters:
    -----------
    context_dict : dict
        A dictionary with string keys and string values representing text content.

    Returns:
    --------
    dict
        The input dictionary with each value transformed into a list of substrings,
        obtained by removing newline characters and splitting the original string by the '*' character,
        excluding the first split element.
    """
    for key in context_dict.keys():
        context_dict[key] = context_dict[key].replace("\n", "").split("*")[1:]
    return context_dict


def encode_dict(sent_dict, model: SentenceTransformer):
    """
    Encodes a dictionary of sentence lists into embeddings using a SentenceTransformer model.

    Args:
        sent_dict (dict): A dictionary where each key maps to a list of sentences (str) to be encoded.
        model (SentenceTransformer): A preloaded SentenceTransformer model used to generate sentence embeddings.

    Returns:
        dict: A new dictionary with the same keys, where each value is a list of corresponding sentence embeddings.
    """
    enc_sent_dict = {}
    for key in sent_dict.keys():
        enc_sent_dict[key] = model.encode(sent_dict[key])
    return enc_sent_dict


def load_stanza(stanza_dir: str, mode: str = "TOKENIZER"):
    """
    Load a Stanza NLP pipeline with the specified processing mode.

    Initializes and returns a Stanza pipeline configured for either Named Entity Recognition (NER)
    or tokenization, depending on the provided mode.

    Args:
        mode (str): Processing mode for the pipeline.
                    Valid values are:
                        - "NER": Load NER processor (default)
                        - "TOKENIZER": Load tokenizer processor only
        stanza_dir (str): Path to the directory containing Stanza resources.
                          Defaults to '/home/mb/stanza_resources/'.

    Returns:
        stanza.Pipeline: A configured Stanza NLP pipeline object.

    Raises:
        Exception: If `mode` is not one of "NER" or "TOKENIZER".
    """
    if mode == "NER":
        processors = {"ner": "i2b2"}
    elif mode == "TOKENIZER":
        processors = "tokenize"
    else:
        raise Exception("mode must be NER or TOKENIZER")

    nlp_tokenized = stanza.Pipeline(
        lang="en",
        package="mimic",
        processors=processors,
        dir=stanza_dir,
        download_method=None,
        verbose=True,  # Enable verbose output to confirm resource loading
    )

    return nlp_tokenized


def getReports(report_dir: str = "path/to/UTI/files"):
    """
    Load and aggregate translated report files from patient subfolders within a directory.

    Parameters:
    -----------
    report_dir : str, optional
        Path to the directory containing patient subfolders with report files.
        Each patient folder should contain text files ending with "translation.txt".

    Returns:
    --------
    tuple of two dicts:
        - report_dict: A nested dictionary where keys are patient folder names and values are
          dictionaries mapping each report filename to its text content (with certain newline
          characters replaced).
        - summary_dict: A dictionary mapping each patient folder name to a concatenated string
          summary of all report contents for that patient.
    """
    patient_folders = os.listdir(report_dir)
    report_dict = {}
    summary_dict = {}
    for folder in patient_folders:
        files = [f for f in os.listdir(os.path.join(report_dir, folder)) if f.endswith("translation.txt")]
        report_dict[folder] = {}
        patient_summary = ""
        for file in files:
            with open(os.path.join(report_dir, folder, file), "r") as content:
                file_content = content.read().replace("\n", " ")
                report_dict[folder][file] = file_content
                patient_summary += file_content
                patient_summary += "\n"
        summary_dict[folder] = patient_summary

    return report_dict, summary_dict


def segment_dict(text_dict, stanza_pipeline):
    """
    Segment texts into sentences using a Stanza pipeline.

    Parameters:
    -----------
    text_dict : dict
        Dictionary where keys are identifiers and values are text strings to be segmented.
    stanza_pipeline : stanza.Pipeline
        An initialized Stanza pipeline configured for sentence segmentation.

    Returns:
    --------
    dict
        Dictionary with the same keys as `text_dict`, where each value is a list of
        sentences (strings) extracted from the corresponding text.
    """
    sent_dict = {}
    for key in text_dict.keys():
        seg_output = stanza_pipeline(text_dict[key])
        sent_dict[key] = [sentence.text for sentence in seg_output.sentences]

    return sent_dict


def split_sents(segment_dict):
    """
    Further splits sentences in the input dictionary at newline characters and filters out short sentences.

    Sentences shorter than or equal to 3 characters are excluded from the results.

    Parameters:
    -----------
    segment_dict : dict
        Dictionary where keys are identifiers and values are lists of sentences (strings).

    Returns:
    --------
    dict
        Dictionary with the same keys as `segment_dict`, where each value is a list of sentences
        split at newline characters and filtered to include only sentences longer than 3 characters.
    """
    split_sent_dict = {}
    for key in segment_dict.keys():
        sent_list = segment_dict[key]
        split_list = [sent.split("\n") for sent in sent_list]
        flattened_split_list = [sub_item for item in split_list for sub_item in item]
        split_sent_dict[key] = [sent for sent in flattened_split_list if len(sent) > 3]

    return split_sent_dict


def load_ground_truth_files_HCY(dir_path):
    """
    Load ground truth CSV files, reading 'hpo_codes' and 'manual_verification' columns into a dictionary.
    Tries to read each file with ';' delimiter first, then ',' if ';' fails.

    Parameters:
    - dir_path (str): Directory path containing ground truth CSV files.

    Returns:
    - gt_dict (dict): Dictionary with file keys and 'hpo_codes' and 'manual_verification' data.
    - failed_files (list): List of files that failed to be read due to errors.
    """
    # Initialize ground truth dictionary and list for tracking failed files
    gt_dict = {}
    failed_files = []

    # List CSV files in the directory
    gt_csv_files = sorted([file for file in os.listdir(dir_path) if file.endswith(".csv")])

    # Process each file in the directory
    for i, file in enumerate(gt_csv_files, start=1):
        file_path = os.path.join(dir_path, file)
        try:
            # Attempt reading with a semicolon delimiter first
            with open(file_path, "r") as f:
                reader = csv.DictReader(f, delimiter=";")
                gt_dict[file[:6]] = {"hpo_codes": [], "manual_verification": []}
                for row in reader:
                    gt_dict[file[:6]]["hpo_codes"].append(row["hpo_codes"])
                    gt_dict[file[:6]]["manual_verification"].append(row["manual_verification"])
        except Exception:
            # If semicolon delimiter fails, try reading with a comodela delimiter
            try:
                with open(file_path, "r") as f:
                    reader = csv.DictReader(f, delimiter=",")
                    gt_dict[file[:6]] = {"hpo_codes": [], "manual_verification": []}
                    for row in reader:
                        gt_dict[file[:6]]["hpo_codes"].append(row["hpo_codes"])
                        gt_dict[file[:6]]["manual_verification"].append(row["manual_verification"])
            except Exception as e:
                # Track files that failed to be read
                failed_files.append(file)
                print(f"Failed to read file {i}/{len(gt_csv_files)}: {file_path} - Error: {e}")

    return gt_dict, gt_csv_files, failed_files


def normalize_hpo_codes(gt_dict):
    """
    Normalize HPO codes in the ground truth dictionary by handling list-like strings
    and replacing curly quotes with straight quotes.

    Parameters:
    - gt_dict (dict): Dictionary containing HPO codes and manual verification data.

    Returns:
    - dict: Updated gt_dict with normalized 'hpo_codes' for each entry.
    """
    for key, data in gt_dict.items():
        normalized_hpo_codes = []
        for code in data["hpo_codes"]:
            # Replace curly quotes with straight quotes
            code = code.replace("‘", "'").replace("’", "'")
            try:
                # Attempt to evaluate it to see if it's a list-like string
                evaluated_code = ast.literal_eval(code)
                # If evaluated_code is a list, take the first element as the plain string
                if isinstance(evaluated_code, list) and evaluated_code:
                    normalized_hpo_codes.append(evaluated_code[0])
                else:
                    # If it's already a plain string, keep it as is
                    normalized_hpo_codes.append(code)
            except (ValueError, SyntaxError):
                # If it fails to evaluate, keep it as a plain string
                normalized_hpo_codes.append(code)

        # correct missing colons
        normalized_hpo_codes = [
            code if ":" in code or not code else code[:2] + ":" + code[2:]
            for code in normalized_hpo_codes  # if code  # This removes empty strings
        ]
        # Update 'hpo_codes' with the normalized list
        gt_dict[key]["hpo_codes"] = normalized_hpo_codes

    return gt_dict


def filter_gt_dict_entries(gt_dict, isin_values, deletion=False):
    """
    Filter out entries in the ground truth dictionary that have an empty HPO code string or 'X' in manual verification
    Optional: Delete files with no entries (if deletion=True)

    Parameters:
    - gt_dict (dict): Ground truth dictionary containing 'hpo_codes' and 'manual_verification' for each file key.
    - isin_values (list): List of values for the 'manual_verification' column to filter HPO codes.
    - deletion (bool): If True, delete files where 'hpo_codes' are empty after filtering.

    Returns:
    - gt_dict (dict): Updated ground truth dictionary after filtering.
    - files_empty (list): List of file keys with no entries (after filtering).
    - files_missingID (list): List of file keys that initially had missing IDs (empty strings) in 'hpo_codes'.
    """
    files_empty = []
    files_missingID = []

    for key, data in list(gt_dict.items()):
        # Filter hpo codes
        filtered_hpo_codes = []
        filtered_manual_verification = []
        for hpo_code, manual_ver in zip(data["hpo_codes"], data["manual_verification"]):
            if manual_ver in isin_values:
                if hpo_code.strip():
                    filtered_hpo_codes.append(hpo_code)
                    filtered_manual_verification.append(manual_ver)
                else:
                    files_missingID.append(key)

        data["hpo_codes"] = filtered_hpo_codes
        data["manual_verification"] = filtered_manual_verification

        # Mark files as empty if 'hpo_codes' is empty after filtering
        entry_deleted = False
        if not data["hpo_codes"]:
            files_empty.append(key)
            if deletion:
                del gt_dict[key]
                entry_deleted = True

        # Convert remaining data to DataFrame if entry was not deleted
        if not entry_deleted:
            gt_dict[key] = pd.DataFrame.from_dict(data)

    return gt_dict, files_empty, sorted(set(files_missingID))


class SymptomScoreCalculator:
    """
    A class for computing similarity scores between input text and symptom-specific context vectors.

    This class uses a provided context dictionary to calculate similarity scores based on a specified
    sentence scoring strategy. It supports selecting either all or the top-N most similar vectors when
    computing mean-based scores.

    Attributes:
        context_dict (dict): A dictionary mapping concept identifiers to lists of context vectors.
        mode (str): Sentence scoring strategy. Options include:
                    - "max": Use the maximum similarity score.
                    - "mean": Use the average of similarity scores.
        top_n (int or str): Number of top context vectors to consider when averaging.
                            Use "all" to include all vectors. Default is "all".

    Args:
        context_dict (dict): The context embedding dictionary for each symptom.
        sentScoring (str, optional): Sentence scoring method ("max", "mean", etc.). Default is "max".
        top_n (int or str, optional): Number of top context vectors to use for averaging.
                                      Use "all" to include all vectors. Default is "all".
    """
    def __init__(self, context_dict, sentScoring: str = "max", top_n="all"):
        self.context_dict = context_dict
        self.mode = sentScoring
        self.top_n = top_n

    def cos_sim(self, A, B):
        """
        Compute the cosine similarity between two vectors.

        Calculates the cosine of the angle between the input vectors A and B,
        which reflects their orientation regardless of magnitude.

        Args:
            A (np.ndarray): First input vector.
            B (np.ndarray): Second input vector.

        Returns:
            float: Cosine similarity score between A and B. Ranges from -1 to 1.
        """
        return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))

    def sent_2_context(self, concept_key, sent):
        """
        Compute a similarity score between a sentence and a concept's context vectors.

        Calculates the similarity between the input sentence vector and all context vectors
        associated with the given concept. The final score is aggregated based on the
        configured mode (e.g., "max", "mean") and the specified number of top context vectors.

        Args:
            concept_key (str): Identifier for the concept in the context dictionary.
            sent (np.ndarray): Vector representation of the sentence to evaluate.

        Returns:
            float: Aggregated similarity score based on the configured mode and top_n.
                   - If `top_n` is "all", all context vectors are considered.
                   - If `top_n` is an integer, only the top-n most similar vectors are used.

        Notes:
            - Supported scoring modes:
                - "max": Use the maximum similarity.
                - "mean": Use the average of all or top-n similarities.
            - Falls back to overall mean if an unsupported mode is provided.
        """
        values = [self.cos_sim(context, sent) for context in self.context_dict[concept_key]]

        if self.mode == "max":
            return max(values)
        elif self.mode == "mean":
            if self.top_n == "all":
                return np.mean(sorted(values)[:])
            elif isinstance(self.top_n, int):
                return np.mean(sorted(values)[-self.top_n:])
        else:
            return np.mean(values)  # Default behavior

    def text_2_context(self, sent_list, concept_key):
        """
        Compute context similarity scores for a list of sentence vectors.

        Evaluates each sentence in the input list by computing its similarity to the context
        vectors associated with the specified concept key. The similarity for each sentence
        is calculated using the configured sentence scoring mode (e.g., "max", "mean").

        Args:
            sent_list (List[np.ndarray]): List of vectorized sentences to be evaluated.
            concept_key (str): Identifier for the concept in the context dictionary.

        Returns:
            List[float]: A list of context similarity scores, one per sentence in the input list.
        """
        return [self.sent_2_context(concept_key, sent) for sent in sent_list]

    def symptom_score(self, concept_key, enc_summary_sent_dict, patient_key):
        """
        Compute the maximum context similarity score for a given patient's sentence vectors.

        This method evaluates all sentence vectors in a patient's summary and returns the
        index and value of the sentence with the highest similarity to the context vectors
        associated with a specific concept.

        Args:
            concept_key (str): Identifier for the concept in the context dictionary.
            enc_summary_sent_dict (Dict[str, List[np.ndarray]]):
                Dictionary mapping patient IDs to lists of sentence vectors.
            patient_key (str): Key identifying the patient to evaluate.

        Returns:
            Tuple[int, float]: Index of the most relevant sentence and its context similarity score.
        """
        sent_list = enc_summary_sent_dict[patient_key]
        text_context = self.text_2_context(sent_list, concept_key)

        max_value = max(text_context)
        max_index = text_context.index(max_value)

        return max_index, max_value

    def check_patient(self, concept_keys, enc_summary_sent_dict, patient_key):
        """
        Calculate symptom scores for multiple concepts for a given patient.

        Iterates over a list of concept keys and computes the symptom score for each
        concept using the patient's encoded summary sentence vectors.

        Args:
            concept_keys (List[str]): List of concept identifiers to evaluate.
            enc_summary_sent_dict (Dict[str, List[np.ndarray]]):
                Dictionary mapping patient IDs to lists of sentence vectors.
            patient_key (str): Identifier for the patient whose data is being scored.

        Returns:
            Dict[str, Dict[str, Union[int, float]]]:
                A dictionary where each concept key maps to another dictionary containing:
                - "max_index": index of the sentence with the highest score
                - "max_value": the highest symptom score for that concept
        """
        patient_check = {}
        for concept_key in concept_keys:
            patient_check[concept_key] = {}
            patient_check[concept_key]["max_index"], patient_check[concept_key]["max_value"] = self.symptom_score(
                concept_key, enc_summary_sent_dict, patient_key
            )

        return patient_check

    def resetSentScoring(self, set_mode: str = "mean", set_top_n="all"):
        """
        Update the sentence scoring mode and number of top context vectors considered.

        Args:
            set_mode (str): Aggregation method for scoring sentences (e.g., 'max', 'mean').
                            Defaults to 'mean'.
            set_top_n (Union[int, str]): Number of top context vectors to include in scoring.
                                         Use 'all' to include all vectors. Defaults to 'all'.
        """
        self.mode = set_mode
        self.top_n = set_top_n

    def add_info(self, summary_sent_dict, patient_key, patient_check):
        """
        Enriches the patient_check dictionary with additional information including the top sentence
        related to each symptom and the symptom's human-readable name from the HPO ontology.

        Parameters:
        -----------
        summary_sent_dict : dict
            Dictionary mapping patient keys to lists of summary sentences.
        patient_key : str
            Key identifying the patient whose data is being processed.
        patient_check : dict
            Dictionary where keys are symptom HPO codes and values contain metadata including
            the index of the top sentence ("max_index").

        Returns:
        --------
        dict
            Updated patient_check dictionary with added keys:
            - "top_sent": the top sentence associated with the symptom.
            - "symptom": the human-readable symptom name fetched from the HPO ontology.
        """
        from util import HPOTree

        hpo_tree = HPOTree()
        data = hpo_tree.data
        for symptom in patient_check.keys():
            top_sent = summary_sent_dict[patient_key][patient_check[symptom]["max_index"]]
            patient_check[symptom]["top_sent"] = top_sent
            patient_check[symptom]["symptom"] = data[symptom]["Name"][0]

        return patient_check

    def symptom_sents(self, enc_summary_sent_dict, summary_sent_dict, symptom_list, top_n=5):
        """
        Selects and returns the top scoring sentences related to each symptom for each patient.

        Parameters:
        -----------
        enc_summary_sent_dict : dict
            Dictionary mapping patient keys to encoded summary sentences used for scoring.
        summary_sent_dict : dict
            Dictionary mapping patient keys to lists of original summary sentences.
        symptom_list : list
            List of symptom HPO codes to filter sentences for.
        top_n : int, optional (default=5)
            Number of top scoring sentences to return per symptom.

        Returns:
        --------
        dict
            Nested dictionary with the following structure:
            {
                patient_key: {
                    symptom: {
                        "top_sents": list of top_n sentences most relevant to the symptom,
                        "top_scores": list of corresponding relevance scores,
                        "symptom name": human-readable name of the symptom from the HPO ontology,
                        "top_indices": list of indices of the top sentences within the original summary
                    }
                }
            }
        """
        from util import HPOTree

        hpo_tree = HPOTree()
        data = hpo_tree.data
        filtered_sents = {}
        for key in enc_summary_sent_dict.keys():
            filtered_sents[key] = {}
            for symptom in symptom_list:
                filtered_sents[key][symptom] = {}
                score_list = self.text_2_context(enc_summary_sent_dict[key], symptom)
                sorted_indices = np.argsort(np.array(score_list))[::-1]
                top_sents = []
                top_scores = []
                top_indices = []
                for index in sorted_indices:
                    if len(top_sents) < top_n:
                        candidate_sent = summary_sent_dict[key][index]
                        if candidate_sent not in top_sents:
                            top_sents.append(candidate_sent)
                            top_scores.append(score_list[index])
                            top_indices.append(index)
                    else:
                        break
                filtered_sents[key][symptom]["top_sents"] = top_sents
                filtered_sents[key][symptom]["top_scores"] = top_scores
                filtered_sents[key][symptom]["symptom name"] = data[symptom]["Name"][0]
                filtered_sents[key][symptom]["top_indices"] = top_indices
        return filtered_sents
